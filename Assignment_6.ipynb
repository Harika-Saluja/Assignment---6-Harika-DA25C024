{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab39894a",
   "metadata": {},
   "source": [
    "# DA5401 - ASSIGNMENT 6\n",
    "## Imputation via Regression for Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6aeeca",
   "metadata": {},
   "source": [
    "#### Importing the useful libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cc332646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad67b252",
   "metadata": {},
   "source": [
    "### Part A: Data Preprocessing and Imputation \n",
    "1. Loading and Preparing Data: Loading the dataset and, artificially introducing MAR missing values (5-10% in 2-3 numerical feature columns). The target variable is 'default payment next month'. \n",
    "2. Imputation Strategy 1: Simple Imputation (Baseline): \n",
    "    - Creating a clean dataset copy (Dataset A). \n",
    "    - For each column with missing values, filling the missing values with the median of that column.\n",
    "3. Imputation Strategy 2: Regression Imputation (Linear): \n",
    "    - Creating a second clean dataset copy (Dataset B). \n",
    "    - For a single column with missing values, using a Linear Regression model to predict the missing values based on all other non-missing features.  \n",
    "4. Imputation Strategy 3: Regression Imputation (Non-Linear): \n",
    "    - Creating a third clean dataset copy (Dataset C). \n",
    "    - For the same column as in Strategy 2, using a non-linear regression model (e.g., K-Nearest Neighbors Regression or Decision Tree Regression) to predict the missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5844a7",
   "metadata": {},
   "source": [
    "#### Loading the Dataset - UCI_Credit_Card.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "260577f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('UCI_Credit_Card.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b79c8eb",
   "metadata": {},
   "source": [
    "#### Artificially Introducing MAR (Missing At Random) missing values - 8% in 3 feature columns with target column - default payment next month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "eb4d3f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values introduced successfully!\n",
      "AGE          2400\n",
      "BILL_AMT1    2400\n",
      "BILL_AMT2    2400\n",
      "dtype: int64\n",
      "\n",
      "Target variable check:\n",
      "default.payment.next.month\n",
      "0    23364\n",
      "1     6636\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Saved as 'UCI_Credit_Card_MAR.csv'\n"
     ]
    }
   ],
   "source": [
    "# Selecting columns to introduce missing values\n",
    "cols_with_missing = ['AGE', 'BILL_AMT1', 'BILL_AMT2']\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Introducing 8% Missing At Random (MAR) values\n",
    "for col in cols_with_missing:\n",
    "    n_missing = int(0.08 * len(df))\n",
    "    missing_indices = np.random.choice(df.index, n_missing, replace=False)\n",
    "    # Introduce NaN in selected positions\n",
    "    df.loc[missing_indices, col] = np.nan\n",
    "\n",
    "print(\"\\nMissing values introduced successfully!\")\n",
    "print(df[cols_with_missing].isna().sum())\n",
    "\n",
    "# Verifying target column presence\n",
    "print(\"\\nTarget variable check:\")\n",
    "print(df['default.payment.next.month'].value_counts())\n",
    "\n",
    "# Saving a copy for further steps\n",
    "df.to_csv('UCI_Credit_Card_MAR.csv', index=False)\n",
    "print(\"\\nSaved as 'UCI_Credit_Card_MAR.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10834472",
   "metadata": {},
   "source": [
    "##### After introducing Missing-At-Random (MAR) values, approximately 8% of the entries (around 2400 records) in the columns AGE, BILL_AMT1, BILL_AMT2 were replaced with NaN to simulate real-world data imperfections. The target variable, default.payment.next.month, remain unchanged with 23,364 non-defaulters and 6,636 defaulters, ensuring fair evaluation of subsequent imputation methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9784cf7",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73d8b1a",
   "metadata": {},
   "source": [
    "#### Imputation 1 - Simple Imputation (Baseline) - Replacing missing values with median of the respective columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9eac1471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values after imputation:\n",
      "AGE          0\n",
      "BILL_AMT1    0\n",
      "BILL_AMT2    0\n",
      "dtype: int64\n",
      "\n",
      "Saved as 'UCI_Credit_Card_MedianImputed.csv'\n"
     ]
    }
   ],
   "source": [
    "# Creating a copy so the original remains unchanged\n",
    "df_A = df.copy()\n",
    "\n",
    "cols_with_missing = ['AGE', 'BILL_AMT1', 'BILL_AMT2']\n",
    "\n",
    "# Replace NaN with median of each column\n",
    "for col in cols_with_missing:\n",
    "    median_value = df_A[col].median()\n",
    "    df_A[col].fillna(median_value, inplace=True)\n",
    "\n",
    "print(\"\\nMissing values after imputation:\")\n",
    "print(df_A[cols_with_missing].isna().sum())\n",
    "\n",
    "df_A.to_csv('UCI_Credit_Card_MedianImputed.csv', index=False)\n",
    "print(\"\\nSaved as 'UCI_Credit_Card_MedianImputed.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309cc149",
   "metadata": {},
   "source": [
    "##### After performing imputation, all missing values in the columns AGE, BILL_AMT1, BILL_AMT2 have been successfully filled, leaving no gaps in the dataset. This means the data is now complete and ready for further analysis or modeling. \n",
    "\n",
    "##### The median was chosen over the mean for the imputation because it provides a more reliable representation of the data when there are outliers or skewed distributions, which are common in financial datasets. The mean can be heavily influenced by extremely high or low values, leading to distorted replacements for the missing data. The median, being the middle value, remains unaffected by such extremes and preserves the natural balance of the dataset. This makes it a more robust and realistic choice for imputing missing values, ensuring that the overall distribution of the data remains consistent and that the resulting model performs more reliably."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b756d6",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed14954",
   "metadata": {},
   "source": [
    "#### Imputation 2 - Regression Imputation (Linear) - Using a linear regression model to predict missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "038e2a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values in AGE column after imputation:\n",
      "0\n",
      "\n",
      "Columns still with missing values: ['BILL_AMT1', 'BILL_AMT2']\n",
      "\n",
      "Remaining missing values after median imputation: 0\n",
      "\n",
      "Saved as 'UCI_Credit_Card_RegressionImputed.csv'\n"
     ]
    }
   ],
   "source": [
    "target_col = 'AGE'\n",
    "df_B = df.copy()\n",
    "\n",
    "# Rows where AGE is missing\n",
    "missing_mask = df_B[target_col].isna()\n",
    "train_data = df_B.loc[~missing_mask]\n",
    "pred_data = df_B.loc[missing_mask]\n",
    "\n",
    "# Selecting numeric columns except the target\n",
    "numeric_cols = df_B.select_dtypes(include=np.number).columns.tolist()\n",
    "numeric_cols.remove(target_col)\n",
    "\n",
    "# Using only columns with no missing values for predictors\n",
    "predictors = [col for col in numeric_cols if df_B[col].isna().sum() == 0]\n",
    "\n",
    "X_train = train_data[predictors]\n",
    "y_train = train_data[target_col]\n",
    "X_pred = pred_data[predictors]\n",
    "\n",
    "# Training Linear Regression\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict missing AGE values\n",
    "predicted_values = reg.predict(X_pred)\n",
    "df_B.loc[missing_mask, target_col] = predicted_values\n",
    "\n",
    "print(\"\\nMissing values in AGE column after imputation:\")\n",
    "print(df_B[target_col].isna().sum())\n",
    "\n",
    "# Filling remaining missing values with median in other columns\n",
    "remaining_missing_cols = df_B.columns[df_B.isna().sum() > 0].tolist() \n",
    "\n",
    "if remaining_missing_cols: \n",
    "    print(\"\\nColumns still with missing values:\", remaining_missing_cols) \n",
    "    for col in remaining_missing_cols: \n",
    "        median_value = df_B[col].median() \n",
    "        df_B[col].fillna(median_value, inplace=True) \n",
    "else: print(\"\\nNo remaining missing columns found.\") \n",
    "\n",
    "print(\"\\nRemaining missing values after median imputation:\", df_B.isna().sum().sum())\n",
    "\n",
    "# Saving clean dataset\n",
    "df_B.to_csv('UCI_Credit_Card_RegressionImputed.csv', index=False)\n",
    "print(\"\\nSaved as 'UCI_Credit_Card_RegressionImputed.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2eca8ef",
   "metadata": {},
   "source": [
    "##### After running the regression-based imputation, the results show that there are no missing values left in the AGE column - meaning the model successfullly filled in all the previously missing entries using predictions based on other available numerical features. This indicates that the regression model captured enough relationships in the data to estimate resonable AGE values. \n",
    "\n",
    "##### This method assumes that the missing AGE values can be predicted from other known features through a straight-line (linear) relationship. It also relies on the idea that the missingness is Missing At Random (MAR) - meaning the reason AGE is missing depends on other observed data, not on AGE itself. When both hold true, linear regression gives realistic and consistent estimates for the missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0077633b",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0be971",
   "metadata": {},
   "source": [
    "#### Imputation 3 - Regression Imputation (Non-Linear) - Using a non-linear regressiion model (K-Nearest Neighbors) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8b1d57df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values in 'AGE' after KNN regression imputation: 0\n",
      "\n",
      "Columns still with missing values: ['BILL_AMT1', 'BILL_AMT2']\n",
      "\n",
      "Remaining missing values after median imputation: 0\n",
      "\n",
      "Saved as 'UCI_Credit_Card_NonLinearImputed.csv'\n"
     ]
    }
   ],
   "source": [
    "df_C = df.copy()\n",
    "\n",
    "target_col = 'AGE'\n",
    "\n",
    "# Identifying rows where AGE is missing\n",
    "missing_mask = df_C[target_col].isna()\n",
    "\n",
    "# Training and prediction subsets\n",
    "train_data = df_C.loc[~missing_mask]\n",
    "pred_data = df_C.loc[missing_mask]\n",
    "\n",
    "# Selecting numeric columns except the target\n",
    "numeric_cols = df_C.select_dtypes(include=np.number).columns.tolist()\n",
    "numeric_cols.remove(target_col)\n",
    "\n",
    "# Keep only columns with no missing values\n",
    "predictors = [col for col in numeric_cols if df_C[col].isna().sum() == 0]\n",
    "\n",
    "# Define training and prediction data\n",
    "X_train = train_data[predictors]\n",
    "y_train = train_data[target_col]\n",
    "X_pred = pred_data[predictors]\n",
    "\n",
    "knn_reg = KNeighborsRegressor(n_neighbors=5)\n",
    "knn_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predicting missing AGE values\n",
    "predicted_values = knn_reg.predict(X_pred)\n",
    "\n",
    "# Filling missing AGE\n",
    "df_C.loc[missing_mask, target_col] = predicted_values\n",
    "\n",
    "print(\"\\nMissing values in 'AGE' after KNN regression imputation:\",\n",
    "      df_C[target_col].isna().sum())\n",
    "\n",
    "# Filling remaining missing values with median in other columns\n",
    "remaining_missing_cols = df_C.columns[df_C.isna().sum() > 0].tolist() \n",
    "\n",
    "if remaining_missing_cols: \n",
    "    print(\"\\nColumns still with missing values:\", remaining_missing_cols) \n",
    "    for col in remaining_missing_cols: \n",
    "        median_value = df_C[col].median() \n",
    "        df_C[col].fillna(median_value, inplace=True) \n",
    "else: print(\"\\nNo remaining missing columns found.\") \n",
    "\n",
    "print(\"\\nRemaining missing values after median imputation:\", df_C.isna().sum().sum())\n",
    "\n",
    "# Saving cleaned dataset\n",
    "df_C.to_csv('UCI_Credit_Card_NonLinearImputed.csv', index=False)\n",
    "print(\"\\nSaved as 'UCI_Credit_Card_NonLinearImputed.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e80a402",
   "metadata": {},
   "source": [
    "##### The result shows that after applying KNN regression imputation, there are no missing values left in the AGE column - which means the algorithm successfully estimated and filled all the previously missing entries. This worked by finding other records in the dataset that were most similar to the rows with missing AGE values - based on other numerical features - and then using the average of those neighbors' ages to fill the gaps.\n",
    "\n",
    "##### The underlying assumption behind this non-linear method is that data points that are similar in their other features (like bill amounts, payments, or credit limits) will also have similar AGE values. KNN doesn't assume a staright-line (linear) relationship between variables; instead, it assumes that local patterns and proximity in the data space can capture complex, possibly non-linear relationships. Essentially, it relies on the idea that 'similar people behave similarly', making it a flexible, data-driven way to handle missing values when linear regression might not capture the full pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb418f2",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------\n",
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cc44b4",
   "metadata": {},
   "source": [
    "### Part B: Model Training and Performance Assessment \n",
    "1. Data Split: For each of the three imputed datasets (A, B, C), splitting the data into training and testing sets. Also, creating a fourth dataset (Dataset D) by simply removing all rows that contain any missing values (Listwise Deletion). Splitting Dataset D into training and testing sets. \n",
    "2. Classifier Setup: Standardizing the features in all four datasets (A, B, C, D) using StandardScaler. \n",
    "3. Model Evaluation: Training a Logistic Regression classifier on the training set of each of the four datasets (A, B, C, D). Evaluating the performance of each model on its respective test set using a full Classification Report (Accuracy, Precision, Recall, F1-score)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c50496e",
   "metadata": {},
   "source": [
    "#### Splitting the data of 3 imputed datasets into training and testing and also creating new dataset by removing all missing values rows and then splitting that too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "98ea8681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after listwise deletion: (23364, 25)\n",
      "Missing values in df_D: 0\n"
     ]
    }
   ],
   "source": [
    "target_col = 'default.payment.next.month' \n",
    "\n",
    "# For df_A\n",
    "X_A = df_A.drop(columns=[target_col])\n",
    "y_A = df_A[target_col]\n",
    "\n",
    "X_A_train, X_A_test, y_A_train, y_A_test = train_test_split(X_A, y_A, test_size=0.2, random_state=42, stratify=y_A)\n",
    "\n",
    "# For df_B\n",
    "X_B = df_B.drop(columns=[target_col])\n",
    "y_B = df_B[target_col]\n",
    "X_B_train, X_B_test, y_B_train, y_B_test = train_test_split(X_B, y_B, test_size=0.2, random_state=42, stratify=y_B)\n",
    "\n",
    "# For df_C\n",
    "X_C = df_C.drop(columns=[target_col])\n",
    "y_C = df_C[target_col]\n",
    "X_C_train, X_C_test, y_C_train, y_C_test = train_test_split(X_C, y_C, test_size=0.2, random_state=42, stratify=y_C)\n",
    "\n",
    "# Drop any row that has at least one NaN\n",
    "df_D = df.dropna()\n",
    "\n",
    "print(\"Shape after listwise deletion:\", df_D.shape)\n",
    "print(\"Missing values in df_D:\", df_D.isna().sum().sum())  \n",
    "\n",
    "X_D = df_D.drop(columns=[target_col])\n",
    "y_D = df_D[target_col]\n",
    "\n",
    "X_D_train, X_D_test, y_D_train, y_D_test = train_test_split(X_D, y_D, test_size=0.2, random_state=42, stratify=y_D)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038374f5",
   "metadata": {},
   "source": [
    "##### The code prepares four differrent versions of the credit card dataset for model training and testing. For datasets A, B, and C - which were imputed using median, linear and non-linear regression methods - the data is split into training and testing sets while keeping the proportion of default and non-default cases balanced. In the final part, we created another version of the dataset, df_D, using listwise deletion, which simply removes any row containing missing values. This output shows that after this process, 23,364 complete records remain with no missing data at all. This means the dataset is now fully clean but smaller in size, as some information was lost when incomplete entries were dropped.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfe70f0",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdd180c",
   "metadata": {},
   "source": [
    "#### Standardizing all the 4 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dd0088fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_A = StandardScaler()\n",
    "\n",
    "# Fit scaler on training set only\n",
    "X_A_train_scaled = scaler_A.fit_transform(X_A_train)\n",
    "X_A_test_scaled = scaler_A.transform(X_A_test)\n",
    "\n",
    "scaler_B = StandardScaler()\n",
    "X_B_train_scaled = scaler_B.fit_transform(X_B_train)\n",
    "X_B_test_scaled = scaler_B.transform(X_B_test)\n",
    "\n",
    "scaler_C = StandardScaler()\n",
    "X_C_train_scaled = scaler_C.fit_transform(X_C_train)\n",
    "X_C_test_scaled = scaler_C.transform(X_C_test)\n",
    "\n",
    "scaler_D = StandardScaler()\n",
    "X_D_train_scaled = scaler_D.fit_transform(X_D_train)\n",
    "X_D_test_scaled = scaler_D.transform(X_D_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b706e2f",
   "metadata": {},
   "source": [
    "#### Training Logistic Regression classifier on all datasets and evaluating the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7bf162b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Logistic Regression on Dataset A (Median Imputation) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.97      0.89      4673\n",
      "           1       0.69      0.24      0.35      1327\n",
      "\n",
      "    accuracy                           0.81      6000\n",
      "   macro avg       0.75      0.60      0.62      6000\n",
      "weighted avg       0.79      0.81      0.77      6000\n",
      "\n",
      "--- Logistic Regression on Dataset B (Linear Regression Imputation) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.97      0.89      4673\n",
      "           1       0.69      0.24      0.35      1327\n",
      "\n",
      "    accuracy                           0.81      6000\n",
      "   macro avg       0.75      0.60      0.62      6000\n",
      "weighted avg       0.79      0.81      0.77      6000\n",
      "\n",
      "--- Logistic Regression on Dataset C (Non-Linear Regression Imputation) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.97      0.89      4673\n",
      "           1       0.69      0.24      0.35      1327\n",
      "\n",
      "    accuracy                           0.81      6000\n",
      "   macro avg       0.75      0.60      0.62      6000\n",
      "weighted avg       0.79      0.81      0.77      6000\n",
      "\n",
      "--- Logistic Regression on Dataset D (Listwise Deletion) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.98      0.89      3635\n",
      "           1       0.74      0.24      0.36      1038\n",
      "\n",
      "    accuracy                           0.81      4673\n",
      "   macro avg       0.78      0.61      0.63      4673\n",
      "weighted avg       0.80      0.81      0.77      4673\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Logistic Regression on Dataset A (Median Imputation) ---\")\n",
    "\n",
    "lr_A = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_A.fit(X_A_train_scaled, y_A_train)\n",
    "\n",
    "y_A_pred = lr_A.predict(X_A_test_scaled)\n",
    "\n",
    "print(classification_report(y_A_test, y_A_pred))\n",
    "\n",
    "print(\"--- Logistic Regression on Dataset B (Linear Regression Imputation) ---\")\n",
    "\n",
    "lr_B = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_B.fit(X_B_train_scaled, y_B_train)\n",
    "\n",
    "y_B_pred = lr_B.predict(X_B_test_scaled)\n",
    "\n",
    "print(classification_report(y_B_test, y_B_pred))\n",
    "\n",
    "print(\"--- Logistic Regression on Dataset C (Non-Linear Regression Imputation) ---\")\n",
    "\n",
    "lr_C = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_C.fit(X_C_train_scaled, y_C_train)\n",
    "\n",
    "y_C_pred = lr_C.predict(X_C_test_scaled)\n",
    "\n",
    "print(classification_report(y_C_test, y_C_pred))\n",
    "\n",
    "print(\"--- Logistic Regression on Dataset D (Listwise Deletion) ---\")\n",
    "\n",
    "lr_D = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_D.fit(X_D_train_scaled, y_D_train)\n",
    "\n",
    "y_D_pred = lr_D.predict(X_D_test_scaled)\n",
    "\n",
    "print(classification_report(y_D_test, y_D_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328c6d9c",
   "metadata": {},
   "source": [
    "##### The results show that all four approaches - median imputation, linear regression imputation, non-linear regression imputation, and listwise deletion - produced very similar model performance. The Logistic Regression classifier was able to identify the majority class (non-default, labeled 0) with high precision and recall across all datasets, achieving around 82% precision and 97-98% recall for that class. However, for the minority class (default, labeled 1), the model struggled, with lower recall (around 24%) despite a precision of about 69-74%.\n",
    "\n",
    "##### Overall accuracy remained around 81% for all datasets, and the weighted averages for F1-score were consistent at approximately 0.77. This indicates that, while the models are good at predicting the majority class, they have difficulty correctly identifying defaults, whih is often expected in imbalanced datasets. Interestingly, the type of imputation - whether median, linear regression, or KNN-based non-linear regression - did not significantly change the classifier's performance, suggesting that all three strategies were sufficient to handle missing values without biasing the results. Likewise deletion also performed similarly, though with slightly fewer total samples, confirming that removing rows with missing values did not drastically alter the overall model behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc24c65e",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e11690f",
   "metadata": {},
   "source": [
    "### Part C: Comparative Analysis \n",
    "1. Results Comparison: Creating a summary table comparing the performance metrics (especially F1-score) of the four models: \n",
    "    - Model A (Median Imputation) \n",
    "    - Model B (Linear Regression Imputation) \n",
    "    - Model C (Non-Linear Regression Imputation) \n",
    "    - Model D (Listwise Deletion) \n",
    "2. Efficacy Discussion: \n",
    "    - Discussing the trade-off between Listwise Deletion (Model D) and Imputation (Models A, B, C). Why might Model D perform poorly even if the imputed models perform worse?\n",
    "    - Which regression method (Linear vs. Non-Linear) performed better and why? Relating this to the assumed relationship between the imputed feature and the predictors. \n",
    "    - Concluding with a recommendation on the best strategy for handling missing data in this scenario, justifying by referencing both the classification performance metrics and the conceptual implications of each method. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c11d53d",
   "metadata": {},
   "source": [
    "#### Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b8716d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance Summary:\n",
      "                                        Model  Accuracy  Precision  Recall  \\\n",
      "0                 Model A (Median Imputation)    0.8077     0.7891  0.8077   \n",
      "1      Model B (Linear Regression Imputation)    0.8073     0.7884  0.8073   \n",
      "2  Model C (Non-Linear Regression Imputation)    0.8078     0.7893  0.8078   \n",
      "3                 Model D (Listwise Deletion)    0.8125     0.8008  0.8125   \n",
      "\n",
      "   F1-Score  \n",
      "0    0.7690  \n",
      "1    0.7687  \n",
      "2    0.7694  \n",
      "3    0.7731  \n"
     ]
    }
   ],
   "source": [
    "datasets_preds = {\n",
    "    \"Model A (Median Imputation)\": (y_A_test, y_A_pred),\n",
    "    \"Model B (Linear Regression Imputation)\": (y_B_test, y_B_pred),\n",
    "    \"Model C (Non-Linear Regression Imputation)\": (y_C_test, y_C_pred),\n",
    "    \"Model D (Listwise Deletion)\": (y_D_test, y_D_pred)\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "# Loop through models and generate metrics\n",
    "for model_name, (y_test, y_pred) in datasets_preds.items():\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Accuracy\": report[\"accuracy\"],\n",
    "        \"Precision\": report[\"weighted avg\"][\"precision\"],\n",
    "        \"Recall\": report[\"weighted avg\"][\"recall\"],\n",
    "        \"F1-Score\": report[\"weighted avg\"][\"f1-score\"]\n",
    "    })\n",
    "\n",
    "# Create a summary DataFrame\n",
    "summary_df = pd.DataFrame(results)\n",
    "print(\"\\nModel Performance Summary:\")\n",
    "print(summary_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c3375c",
   "metadata": {},
   "source": [
    "##### Looking at the performance summary, all 4 models show fairly similar results, with overall accuracy hovering around 81%. Among them, Model D, which used listwise deletion, slightly outperforms the other across all metrics, achieving the highest accuracy (0.8125), precision (0.8008), recall (0.8125), and F1-score (0.7731). The median and regression imputation models (A, B, and C) perform almost equally, with only tiny differences in F1-scores and other metrics. This suggests that while imputing missing values - whether with median, linear regression, or non-linear methods - produces solid models, removing rows with missing data (listwise deletion) in this case gave a marginally better predictive performance. Overall, the models are consistent, and the choice of imputation strategy does not drastically change the outcomes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f605398",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc94f92",
   "metadata": {},
   "source": [
    "#### Trade-off between Model D and other 3 Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1ae43a",
   "metadata": {},
   "source": [
    "##### Listwise deletion (Model D) removes any row that contains a missing value, which gurantees a complete dataset for modeling. This approach often simplifies processing and avoids the risks of incorrectly estimating values. In this case, Model D slightly outperformed the imputated models in terms of accuracy, precision, recall, and F1-score. However, the trade-off is that it discards data; here, about 1000+ rows were removed, which reduces the amount of training information and could lead to biased models if the missingness is not completely random. Imputation strategies, on the other hand, preserve all data, leveraging available patterns to estimate missing values. This is why even though the imputated models (A, B, C) performed slightly worse, they maintain the full dataset and often provide more robust insights, especially in real-world scenarios where losing data could hide important patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4d3048",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adc283d",
   "metadata": {},
   "source": [
    "#### Better regression method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dc4425",
   "metadata": {},
   "source": [
    "##### Between the two regression-based imputations, the non-linear method (Model C using KNN) performed marginally better than the linear regression method (Model B) across all performance metrics - showing a slightly higher accuracy (0.8078 vs 0.8073), precision (0.7893 vs 0.7884), recall (0.8078 vs 0.8073), and F1-score (0.7694 vs 0.7687).\n",
    "\n",
    "##### The difference, though small, indicates that the relationship between the imputed feature (AGE) and its predictors is likely non-linear in nature. Linear regression assumes a straight-line relationship where changes in predictors affect the target proportionally. However, human-related variables like age often interact with socio-economic and occupational factors in more complex, curved, or clustered patterns.\n",
    "\n",
    "##### The non-linear approach (such as KNN-based imputation) does not impose a linear structure; instead, it relies on similarity across multidimensional feature space, capturing localized patterns and subtle dependencies that a linear model would oversimplify. Consequently, this method yields imputations that are closer to the true underlying distribution of the missing values, leading to a slightly mode acurate and generalizable model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca5de0f",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3342601",
   "metadata": {},
   "source": [
    "#### Best recommended strategy "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3c23e8",
   "metadata": {},
   "source": [
    "##### Considering both the classification metrics and conceptual implications, imputation using the non-linear regression method (Model C) is the recommended strategy for this scenario. Although listwise deletion showed slightly higher F1-score and accuracy, it comes at the cost of discarding a significant portion of the data. Non-linear imputation retains the full dataset, respects potential complex relationships among features, and delivers nearly the same predictive performance as listwise deletion. This makes it a safer and more generalizable approach, particularly in datasets where missingness is MAR and maintaining data integrity is important. Median or linear regression imputation could also work reasonably well, but KNN-based non-linear imputation better adapts to the data's inherent structure and preserves subtle patterns that might be lost with simpler methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509deb9f",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------\n",
    "----------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
